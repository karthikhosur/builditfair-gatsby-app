---
title:  Can we rely on our computers to predict Ailments?
category: Case Studies
author: Karthik Hosur
date: 2021-04-20
---
## Recent study

In 2015, a research group at Mount Sinai Hospital in New York worked to apply deep learning to the institutionâ€™s large database of patient records featuring hundreds of variables on patients drawn from their test results, doctor visits, etc. The so-called Deep Patient software was trained using data from about 700,000 patients, and when evaluated with new records, its accuracy in predicting disease was exceptional. The system was able to discover patterns hidden in the hospital data that seemed to indicate when patients were on the way to a wide range of ailments, including cancer.

But at the same time, Deep Patient turned out to be somewhat of a black box. For example, it was able to anticipate the onset of psychiatric disorders like schizophrenia surprisingly well. But since schizophrenia is notoriously difficult for physicians to predict, it was natural to wonder how this was possible. Unfortunately, the new tool offered no clue as to how it does this. If a technology solution like Deep Patient is actually going to assist doctors, it really needs to offer a level of transparency by offering a rationale for its predictions to reassure them that it is accurate and to justify any changes in prescription drugs a patient is taking.

## How to handle it

To build a dependable system based on AI, it is crucial to prioritize transparency and explainability in the development process. This means designing algorithms and models that are capable of providing a clear rationale for their predictions or recommendations. One way to achieve this is to incorporate techniques such as interpretability and transparency, which enable users to understand how the system is making decisions and what factors are contributing to its outputs.

Another approach is to establish clear standards for evaluating the accuracy and reliability of AI systems, such as through the use of independent validation and testing. This can help identify any potential sources of bias or errors in the system and ensure that it is providing accurate and dependable results.

Additionally, it is important to involve experts and stakeholders from diverse fields, including medicine, ethics, and computer science, in the development of AI systems. This can help ensure that the system is designed with a comprehensive understanding of the complex issues involved and that it is aligned with ethical and social values.

## Conclusion

Ultimately, building a dependable AI system requires a multi-disciplinary approach that prioritizes transparency, interpretability, and accountability. By doing so, we can create systems that are capable of providing accurate and reliable predictions and recommendations while also ensuring that they are aligned with human values and ethical principles.


