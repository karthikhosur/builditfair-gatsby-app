---
title:  Can an autonomous vehicle be racist?
category: Case Studies
author: Karthik Hosur
date: 2020-04-20
---
## Recent study

Recent mainstream news reports suggested that autonomous cars are unlikely to detect pedestrians crossing the road if they have dark skin and thus run them over. The academic paper, “Predictive Inequity in Object Detection,” by a group of researchers at the Georgia Institute of Technology, highlighted the matter with a series of experiments testing different deep learning computer vision models, such as the Faster R-CNN model and R-50-FPN, on images of pedestrians with different skin tones. The study described how the researchers enlisted the help of human classifiers to look through the collection of roughly 3,500 images, and assign labels as either “LS” for light skin or “DS” for dark skin, and then trained the neural network model using this data set. There was an attempt to ensure the manual classification process was not tainted by any cultural biases. The group found that their models found it difficult to detect people with dark skin, which led them to the conclusion: “This study provides compelling evidence of the real problem that may arise if this source of capture bias is not considered before deploying these sort of recognition models. How do build standards and hold computers accountable for its Bais?

## How to handle it

To address bias in AI, it is important to develop and implement standards for data collection, model development, and testing that prioritize fairness, transparency, and accountability. One way to mitigate bias in autonomous cars is to diversify training data to ensure that it accurately represents different skin tones, ethnicities, genders, and other demographic factors. This will help prevent the development of AI models that disproportionately fail to recognize or respond to individuals from certain groups.

Another approach to addressing bias is to test AI models rigorously to identify and correct any potential sources of bias. This includes examining how different groups are represented in the data used to train the model and assessing the accuracy and fairness of the model's outputs. If bias is detected, the model should be adjusted accordingly.

Additionally, it is crucial to involve diverse stakeholders in the development and deployment of AI systems to ensure that they are designed with a comprehensive understanding of the complex social and ethical issues involved. This includes engaging with individuals and groups who may be impacted by the technology, such as members of marginalized communities, and incorporating their perspectives and feedback into the development process.

## Conclusion

Ultimately, holding computers accountable for bias requires a multi-disciplinary approach that prioritizes responsible and ethical practices throughout the entire AI development process. This includes developing clear standards and guidelines for building and testing AI models, as well as ensuring that these models are subject to ongoing evaluation and review to identify and correct any potential sources of bias.



